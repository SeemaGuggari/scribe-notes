\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{graphicx}
\usepackage{url}

%
% The following sets up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\dnl}{\mbox{}\par}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
  \newpage
  \setcounter{lecnum}{#1}
  \setcounter{page}{1}
  \noindent
  \begin{center}
  \framebox{
     \vbox{\vspace{2mm}
   \hbox to 6.28in { {\bf CMPSCI~630~~~Concurrency
                       \hfill Fall 2014} }
      \vspace{4mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1  \hfill} }
%       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
      \vspace{2mm}
      \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
     \vspace{2mm}}
  }
  \end{center}
  \markboth{Lecture #1: #2}{Lecture #1: #2}
  \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
%
\renewcommand{\cite}[1]{[#1]}

% \input{epsf}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{FIGURE-SIZE}{CAPTION}{FILENAME}
\newcommand{\fig}[4]{
           \vspace{0.2 in}
           \setlength{\epsfxsize}{#2}
           \centerline{\epsfbox{#4}}
           \begin{center}
           Figure \thelecnum.#1:~#3
           \end{center}
   }

% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% Some useful equation alignment commands, borrowed from TeX
\makeatletter
\def\eqalign#1{\,\vcenter{\openup\jot\m@th
 \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
     \crcr#1\crcr}}\,}
\def\eqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\llap{$##$}\tabskip\z@skip\crcr
   #1\crcr}}
\def\leqalignno#1{\displ@y \tabskip\@centering
 \halign to\displaywidth{\hfil$\displaystyle{##}$\tabskip\z@skip
   &$\displaystyle{{}##}$\hfil\tabskip\@centering
   &\kern-\displaywidth\rlap{$##$}\tabskip\displaywidth\crcr
   #1\crcr}}
\makeatother

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:



% Some general latex examples and examples making use of the
% macros follow.

\begin{document}

%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{7}{September 23}{Emery Berger}{Adam Nelson, Seema Guggari}

\section{Review of Experiences with Processes and Monitors in Mesa}

The first paper: half the class thought it was great, half the class hated it.

Why was it significant? People didn't really know what to do with abstractions like threads. The way that things evolved weren't necessarily the best way to structure them. Mesa tried to iron out the semantics of the synchronization primitives.

One of the biggest challenges in PL is interaction of features. For example: if you have objects, then synchronization, then exceptions... how do they interact? This is why designing good languages is hard, and what motivates the use of formal methods. When people design languages in academia, they design these languages using formal methods to prove behavior. Without this, undefined behavior is common. Behavior is often "defined by the compiler," which used to be OK, but can lead to bugs becoming part of the standard.

Mesa is a concurrent based programming language. It was introduced to overcome the shortcomings of monitors. There were several design constraints of monitors. For example, WAIT was not properly designed in a nested monitor call, there was no proper synchronization between monitors and also there was no mechanism to handle exceptions.


\section{Evolution of Threads.}

When a process is created, it has its own address space, PC, register s and stack. Each of these processes are added to the Wait Queue and the scheduler executes them one at a time. 

Process switching is accomplished with a queue; the OS takes a process off the queue, runs it for a bit, then puts it back on the queue.  But in case of multiple processes, the scheduler would have many processes waiting on it and thus the CPU’s performance is degraded.  Hence a time scheduling approach was proposed also known as preemptive scheduling.

In Preemptive scheduling, the process continues executing until a time scheduler interrupts it since the process quantum expires, stores the contents of the PC, stack, registers and address space into a Process Control Block. Retrieves the next process from the Wait Queue and executes it for the same amount of time.

But, we run into one problem: when is a process "done"?

- time slicing \newline 
- blocking \newline 
- on exit \newline 

For example, if a process tries to read from a tape, this will take a long time, so the OS should put it back on the queue and let something else run. This happens if the call is \_synchronous\_, like read().

write() is usually asynchronous, although fsync() is *supposed* to make it synchronous.

But what if a thread just runs, without blocking? In an infinite loop? Other processes could be waiting (keyboard requests, browser)... so we cut it off after a set amount of time (time slicing, aka preemptive scheduling). Processes are stopped after a period of time known as a "quantum," or on a blocking call, whichever comes first.

A hardware timer sends an interrupt signal to the scheduler, which stores the process's state in the queue, and then restores the next process's state from the queue.

     quantum expiration\newline 
            |
process ----|----> save state -----> load state from next proc on queue -----> run it


In order to avoid context switch, some systems are designed to use Non preemptive scheduling, also known as “Cooperative Multitasking”.  This is not used anymore, here the threads tell the scheduler when they can pause ("yield"). Yield does not mean a program is done, just that it can take a break. The process interrupts itself instead of waiting for quantum expiration and gives control to other process. Cooperative Scheduling hogs the CPU completely for a single task.

Coop is the optimist's view, Preemptive is the pessimist's view.

for (i = 0; i < 1e9; i++)\newline
  ... \newline
  yield() \newline

Round-robin scheduling: everyone gets a turn in a fixed order, no prioritization. A FIFO queue is round-robin. Round-robin scheduling is nice, everyone understands it... and no one uses it.

Co-op multitasking is nice, but impractical. Consider the "threat model": it's easy for a malicious program to never yield, and take up the whole CPU. A careless programmer might also do this. An infinite loop can take down the machine.

UNIX machines use preemptive scheduling with optional co-op scheduling. The "hint" sched\_yield() is like a yield.

Co-op scheduling was originally used due to lack of hardware interrupts. Computers didn't have timers!

Preemptive scheduling and cooperative scheduling differ in a way that makes coop scheduling look really nice. Cooperative scheduling is more general than preemptive scheduling

Let's think about it in terms of the possible schedules:

a = 1; \newline
b = 2; \newline
c = 3; \newline
sched\_yield(); \newline

How many ways are there of putting in breaks in this program? \newline
In cooperative: only one, the sched\_yield. \newline
In preemptive: anywhere. \newline

Now, let's say we have threads. The difference between processes and threads: threads can share an address space. (Threads are "lightweight processes." Switching is faster when the address space is the same.)

Cooperative Scheduling  \newline

Thread 1 \hspace{30 mm}                                  Thread 2 \newline
a  =  1;   \hspace{36 mm}                                    a = 2;\newline
b  =  2;   \hspace{36 mm}                                    b = 4;\newline
c  =  3;   \hspace{36 mm}                                    c = 6;\newline
sched\_yeild() \hspace{26 mm}                                 sched\_yield()\newline
print(a,b,c)\newline

Output\newline
If thread 2 executes first : 123\newline
If thread 1 executes first : 246\newline


Preemptive Scheduling \newline

 Thread 1 \hspace{30 mm}                   	Thread 2 \newline
  a = 1;  \hspace{36 mm}                       a = 2;  \newline
  b = 2;  \hspace{36 mm}                       b = 4;  \newline
  c =  3; \hspace{36 mm}                       c  = 6;  \newline
 print(a,b,c)\newline
 
Output \newline
Uncertain. There can be many interleavings and hence mutiple combinataions. As the mumber of threads increase, there are more possiblities. \newline

What a process stores when unscheduled: PC, registers, stack, file descriptors, and the mapping from virtual to physical addresses (page table). This is why threads are lighter-weight than processes: the page table remains the same, file descriptors remain the same, and the hardware cache (TLB) of the page table isn't invalidated. Keeping the cache is the most important part, performance-wise.

\section{Translation Look Aside Buffer (Hash Table) }
TLB is a cache which has the page table entries to map the virtual address to physical address. This cache can be designed in various ways: \newline
\newline
Fully Associative LRU Cache: \newline
  This has almost 512 slots available to be filled for every virtual to physical addressing mapping. This type of cache is      
  sometimes referred to as direct mapping and is fully associative.  From hardware perspective, it’s an expensive cache.  If no  slot is      available in the LRU, it swaps the contents of the least recently used with the new entry. This situation is called "fully associative". Requires a huge hardware queue of the cache entries.\newline
\newline
 1 way and 2 way Set Associative LRU Cache: \newline
  Here the caches have 8 slots available, but 2 way caches have a depth of 2. Scheduler computes the slot where each new entry has to be written via (pg/4096) % 8.  If no space is   available for a new entry, the contents already present in the corresponding slot are directly scrapped out for the new entry.   This cache has the lowest  hardware cost.\newline
\newline

1 way set associative and 2 way set associative caches are not as nice. They do something that's kind of a hack. Suppose I go look up a page. Instead of doing LRU, all they do is hash the page entry to some bucket. If there's nothing there, they can store it there; if there's something, it can evict it. So now this is basically a hash table.\newline

TLB Miss: 

In the beginning, all the caches are cold which means virtual to physical address mapping is not found, TLB later establishes it Hence initially things are slow, then it gets faster. This is known as warm up.

For every single access, program checks the page table for its physical address map. If requested entry is not found in the page table, it is a TLB miss and the OS walks through every address in RAM to compute the physical address. This involves more number of cycles when compared to accessing address from TLB. If we frequently encounter TLB miss, it leads to TLB Thrashing which degrades performance.


\end{document}
